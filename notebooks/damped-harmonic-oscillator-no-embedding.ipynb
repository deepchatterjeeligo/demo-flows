{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\ddot{x}(t) + 2\\beta\\omega_0\\dot{x}(t) + \\omega_0^2x(t) = 0\n",
    "\\end{align}\n",
    "\n",
    "Ansatz, $x = \\exp(\\gamma t)$; $\\gamma = -\\omega_0\\left[\\beta \\pm i\\sqrt{1 - \\beta^2}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damped_sho(t, omega_0, beta, shift=0):\n",
    "    # beta less than 1 for underdamped\n",
    "    envel = beta * omega_0\n",
    "    osc = np.sqrt(1 - beta**2) * omega_0\n",
    "    tau = t - shift\n",
    "    data = np.exp(-envel * tau) * np.cos(osc * tau)\n",
    "    data[tau < 0] = 0  # assume oscillator starts at tau = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_0 = 1\n",
    "beta = 0.3\n",
    "\n",
    "t_vals = np.linspace(-1, 10, 200)\n",
    "for shift in [-1, 0, 1, 2]:\n",
    "    x_vals = damped_sho(t_vals, omega_0=omega_0, beta=beta, shift=shift)\n",
    "    plt.plot(t_vals, x_vals, label=f\"{omega_0=}; {beta=}; {shift=}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Disp\")\n",
    "plt.axvline(x=0, linestyle='dotted', color='black')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_parameters = dict(omega_0=omega_0, beta=beta, shift=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "t_vals = np.linspace(-1, 10, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = damped_sho(t_vals, **injection_parameters) + np.random.normal(0, sigma, t_vals.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(t_vals, data, 'o', label='$\\\\{x_i, y_i\\\\}$')\n",
    "ax.plot(t_vals, damped_sho(t_vals, **injection_parameters), '--r', label='f(x)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bilby\n",
    "from bilby.core.prior import Uniform, DeltaFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = dict()\n",
    "\n",
    "priors['omega_0'] = Uniform(0.1, 2, name='omega_0', latex_label='$\\omega_0$')\n",
    "priors['beta'] = Uniform(0, 0.5, name='beta', latex_label='$\\\\beta$')\n",
    "priors['shift'] = Uniform(-4, 4, name='shift', latex_label='$\\Delta\\;t$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bilby.core.likelihood import GaussianLikelihood\n",
    "\n",
    "# log_l = GaussianLikelihood(t_vals, data, damped_sho, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# result = bilby.run_sampler(\n",
    "#     likelihood=log_l, priors=priors, sampler='dynesty',\n",
    "#     nlive=300, npool=4, save=False, clean=True,\n",
    "#     injection_parameters=injection_parameters,\n",
    "#     outdir='./damped_sho',\n",
    "#     label='damped_sho'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.plot_corner(priors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embedding space first\n",
    "\n",
    "The aim is to make the network be agnostic to shift in time. Train embedding space to cluster shifts in intercept value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(omega_0=None, beta=None, shift=None, num_points=1):\n",
    "    \"\"\"Sample omega, beta, shift and return a batch of data with noise\"\"\"\n",
    "    omega_0 = priors['omega_0'].sample() if omega_0 is None else omega_0\n",
    "    beta = priors['beta'].sample() if beta is None else beta\n",
    "    shift = priors['shift'].sample() if shift is None else shift\n",
    "    t_vals = np.linspace(-1, 10, num_points)\n",
    "    y = damped_sho(t_vals, omega_0=omega_0, beta=beta, shift=shift)\n",
    "    y += sigma*np.random.normal(size=y.size)\n",
    "\n",
    "    return t_vals, y, omega_0, beta, shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 10000\n",
    "theta_vals = []\n",
    "data_vals = []\n",
    "for ii in range(num_simulations):\n",
    "    t_vals, y, omega, beta, shift = get_data(num_points=200, shift=0)\n",
    "    data_vals.append(y)\n",
    "    theta_vals.append([omega, beta, shift])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_vals = np.array(theta_vals)\n",
    "data_vals = np.array(data_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    def __len__(self):\n",
    "        return num_simulations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # augment the data to have several shifted intercepts\n",
    "        theta, data_aug, data_orig = self.augment(idx)\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(theta).to(torch.float32),\n",
    "            torch.from_numpy(data_aug).to(torch.float32),\n",
    "            torch.from_numpy(data_orig).to(torch.float32)\n",
    "        )\n",
    "\n",
    "    def augment(self, idx):\n",
    "        repeats = 30  # just a choice, can be different\n",
    "        omega = theta_vals[idx][0]\n",
    "        beta = theta_vals[idx][1]\n",
    "        theta_aug_vals, data_aug_vals, data_orig_vals = [], [], []\n",
    "        for _ in range(repeats):\n",
    "            # reproduce every instance with zero shifts\n",
    "            t_val, y_val_orig, omega_0, beta, shift = get_data(\n",
    "                omega_0=omega, beta=beta, shift=0,\n",
    "                num_points=num_points\n",
    "            )\n",
    "            data_orig_vals.append(y_val_orig)\n",
    "            # augment every instance of natural freq and damping with several shifts\n",
    "            t_val, y_val, omega_0, beta, shift = get_data(\n",
    "                omega_0=omega, beta=beta, shift=None,\n",
    "                num_points=num_points\n",
    "            )\n",
    "            data_aug_vals.append(y_val)\n",
    "\n",
    "            theta_aug_vals.append([omega_0, beta, shift])\n",
    "        return np.array(theta_aug_vals), np.array(data_aug_vals), np.array(data_orig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, d, _ = dataset.augment(4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for (omega, beta, shift), points in zip(t, d):\n",
    "    ax.plot(t_vals, points, 'o', markersize=0.8, label='$\\\\{x_i, y_i\\\\}$')\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(f\"Augmented sample; Omega_0 = {t[0][0]:.1f}; Beta = {t[0][1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = int(0.8 * num_simulations)\n",
    "val_set_size = int(0.1 * num_simulations)\n",
    "test_set_size = int(0.1 * num_simulations)\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "    dataset, [train_set_size, val_set_size, test_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 200\n",
    "VAL_BATCH_SIZE = 50\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_data, batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_data, batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theta, data_aug, data_orig in train_data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.shape, data_aug.shape, data_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No similarity embedding in MAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.distributions import StandardNormal\n",
    "from nflows.flows import Flow\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms import CompositeTransform, RandomPermutation\n",
    "from nflows.transforms import IdentityTransform, NaiveLinear\n",
    "\n",
    "import nflows.utils as torchutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_transforms = 2\n",
    "num_transforms = 5\n",
    "num_blocks = 4\n",
    "hidden_features = 20\n",
    "\n",
    "# embedding_net = IdentityTransform() # is default; Will increate number of parameters\n",
    "embedding_net = nn.Linear(200, 2)\n",
    "\n",
    "context_features = embedding_net(data_aug).shape[-1]\n",
    "\n",
    "base_dist = StandardNormal([3])\n",
    "\n",
    "transforms = []\n",
    "for _ in range(num_transforms):\n",
    "    block = [\n",
    "        MaskedAffineAutoregressiveTransform(\n",
    "            features=3,  # 3-dim posterior\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=context_features,\n",
    "            num_blocks=num_blocks,\n",
    "            activation=torch.tanh,\n",
    "            use_batch_norm=False,\n",
    "            use_residual_blocks=True,\n",
    "            dropout_probability=0.01\n",
    "        ),\n",
    "        RandomPermutation(features=3)\n",
    "    ]\n",
    "    transforms += block\n",
    "\n",
    "transform = CompositeTransform(transforms)\n",
    "\n",
    "flow = Flow(transform, base_dist, embedding_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of trainable parameters: \", sum(p.numel() for p in flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num_transforms = 2\n",
    "# flow_model_path = './sho_flow_model_no_similarity_embedding.pt'\n",
    "# torch.save(flow.state_dict(), flow_model_path)\n",
    "# flow.load_state_dict(torch.load(flow_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_augmentations = 1\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(train_data_loader, 1):\n",
    "        augmented_theta, augmented_data, orig_data = val\n",
    "        loss = 0\n",
    "        for ii in range(num_augmentations):\n",
    "            theta = augmented_theta[:,ii,:]\n",
    "            data = augmented_data[:,ii,:]\n",
    "            flow_loss = -flow.log_prob(theta, context=data).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            flow_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss += flow_loss.item()\n",
    "\n",
    "        running_loss += loss/num_augmentations\n",
    "        if idx % 10 == 0:\n",
    "            last_loss = running_loss / 10 # avg loss\n",
    "            print(' Avg. train loss/batch after {} batches = {:.4f}'.format(idx, last_loss))\n",
    "            tb_x = epoch_index * len(train_data_loader) + idx\n",
    "            tb_writer.add_scalar('Flow Loss/train', last_loss, tb_x)\n",
    "            tb_writer.flush()\n",
    "            running_loss = 0.\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def val_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for idx, val in enumerate(val_data_loader, 1):\n",
    "        augmented_theta, augmented_data, orig_data = val\n",
    "        loss = 0\n",
    "        for ii in range(num_augmentations):\n",
    "            theta = augmented_theta[:,ii,:]\n",
    "            data = augmented_data[:,ii,:]\n",
    "\n",
    "            flow_loss = -flow.log_prob(theta, context=data).mean()\n",
    "            loss += flow_loss.item()\n",
    "\n",
    "        running_loss += loss/num_augmentations\n",
    "        if idx % 10 == 0:\n",
    "            last_loss = running_loss / 10\n",
    "            tb_x = epoch_index * len(val_data_loader) + idx + 1\n",
    "            tb_writer.add_scalar('Flow Loss/val', last_loss, tb_x)\n",
    "\n",
    "            tb_writer.flush()\n",
    "            running_loss = 0.\n",
    "    tb_writer.flush()\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(flow.parameters(), lr=2e-3)\n",
    "\n",
    "scheduler_1 = optim.lr_scheduler.ConstantLR(optimizer, total_iters=5)\n",
    "scheduler_2 = optim.lr_scheduler.OneCycleLR(optimizer, total_steps=50, max_lr=2e-3)\n",
    "scheduler_3 = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "scheduler = optim.lr_scheduler.SequentialLR(\n",
    "    optimizer, schedulers=[scheduler_1, scheduler_2, scheduler_3], milestones=[5, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"sho_flow_no_embedding_num_transforms_5\", comment=\"With LR=2e-3\", flush_secs=5)\n",
    "epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# UNCOMMENT AND RUN TO TRAIN FROM SCRATCH\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    # Gradient tracking\n",
    "    flow.train(True)\n",
    "    # flow._embedding_net.train(False)\n",
    "    # no gradient tracking for embedding layer\n",
    "    for name, param in flow._embedding_net.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    avg_train_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # no gradient tracking, for validation\n",
    "    flow.train(False)\n",
    "    avg_val_loss = val_one_epoch(epoch_number, writer)\n",
    "\n",
    "    print(f\"Train/Val flow Loss after epoch: {avg_train_loss:.4f}/{avg_val_loss:.4f}\")\n",
    "\n",
    "    epoch_number += 1\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import corner\n",
    "\n",
    "def cast_as_bilby_result(samples, truth):\n",
    "    injections = dict.fromkeys(injection_parameters)\n",
    "    injections['omega_0'] = float(truth.numpy()[0])\n",
    "    injections['beta'] = float(truth.numpy()[1])\n",
    "\n",
    "    posterior = dict.fromkeys(injection_parameters)\n",
    "    samples_numpy = samples.numpy()\n",
    "    posterior['omega_0'] = samples_numpy.T[0].flatten()\n",
    "    posterior['beta'] = samples_numpy.T[1].flatten()\n",
    "    posterior = pd.DataFrame(posterior)\n",
    "    \n",
    "    return bilby.result.Result(\n",
    "        label=\"test_data\",\n",
    "        injection_parameters=injections,\n",
    "        posterior=posterior,\n",
    "        search_parameter_keys=list(injections.keys()),\n",
    "        priors=priors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_plot_samples(samples, truth):\n",
    "    clear_output(wait=True)\n",
    "    sleep(0.5)\n",
    "    figure = corner.corner(\n",
    "        samples.numpy(), quantiles=[0.16, 0.5, 0.84],\n",
    "        show_titles=True, labels=[\"omega_0\", \"beta\", \"shift\"],\n",
    "        truth=truth\n",
    "    )\n",
    "\n",
    "    corner.overplot_lines(figure, truth, color=\"C1\")\n",
    "    corner.overplot_points(figure, truth[None], marker=\"s\", color=\"C1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, (theta_test, data_test, data_orig) in enumerate(test_data):\n",
    "    if idx % 3 !=0: continue \n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(3000, context=data_test[0].reshape((1, 200)))\n",
    "    live_plot_samples(samples[0], theta_test[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PP plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, (theta_test, data_test, data_unshifted) in enumerate(test_data):\n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(3000, context=data_test[0].reshape((1, 200)))\n",
    "    results.append(\n",
    "        cast_as_bilby_result(samples, theta_test[0]))\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    bilby.result.make_pp_plot(results, save=False, keys=['omega_0', 'beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = pd.concat([r.posterior.std() for r in results], axis=1).T.drop('shift', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = stds[stds.omega_0 < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "stds.omega_0.hist(cumulative=True, density=True, ax=ax, label='$\\Delta\\omega_0$')\n",
    "stds.beta.hist(cumulative=True, density=True, ax=ax, label='$\\Delta\\\\beta$')\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of measurement widths\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
